{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task 5 - Viral Tweets Prediction Challenge\n\nDébora Mayumi Rissato - 5288223\n\nDouglas Decicino de Andrade - 10883512\n\nPaulino Ribeiro Villas Boas - 2950178\n\nRenan Silva Chun - 10691817\n\nRenan de Oliveira da Cruz - 10801090","metadata":{"id":"OgGJwnTlk6PR"}},{"cell_type":"markdown","source":"Notebook com todas as saídas no link: https://github.com/nan-oliveira/ML/blob/main/T05%20-%20Twitter/Task5_final_tweets.ipynb","metadata":{}},{"cell_type":"markdown","source":"Notebook contendo a análise dos dados da competição \"Viral Tweets Prediction Challenge\" (https://bitgrit.net/competition/12). O objetivo desta competição é desenvolver um modelo de aprendizado de máquina para prever o nível de \"viralidade\" de cada tweet com base em atributos como conteúdo do tweet, mídia anexada ao tweet e data/hora de publicação.","metadata":{"id":"SMJsUPCYk6PS"}},{"cell_type":"markdown","source":"Abaixo temos o import de algumas bibliotecas que serão utilizadas no decorrer do código.","metadata":{"id":"SST68e55k6QM"}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import (RepeatedStratifiedKFold, cross_validate,\n                                     train_test_split, GridSearchCV,cross_val_score)\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nimport random\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","metadata":{"id":"s4A1enMuk6QM","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dados treino\ntrain_tweets = pd.read_csv(\"Dataset/Tweets/train_tweets.csv\")\ntrain_tweets_vectorized_media = pd.read_csv(\"Dataset/Tweets/train_tweets_vectorized_media.csv\")\ntrain_tweets_vectorized_text = pd.read_csv(\"Dataset/Tweets/train_tweets_vectorized_text.csv\")\n\n#Dados teste\ntest_tweets = pd.read_csv(\"Dataset/Tweets/test_tweets.csv\")\ntest_tweets_vectorized_media = pd.read_csv(\"Dataset/Tweets/test_tweets_vectorized_media.csv\")\ntest_tweets_vectorized_text = pd.read_csv(\"Dataset/Tweets/test_tweets_vectorized_text.csv\")","metadata":{"id":"NRPuGUNok6QO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_vectorized_descriptions = pd.read_csv(\"Dataset/Users/user_vectorized_descriptions.csv\")\nuser_vectorized_profile_images = pd.read_csv(\"Dataset/Users/user_vectorized_profile_images.csv\")\nusers = pd.read_csv(\"Dataset/Users/users.csv\")","metadata":{"id":"JOQj1gnSk6QO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_tweets_vectorized_media.shape)\nprint(train_tweets_vectorized_text.shape)\nprint(train_tweets.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Abaixo verificamos se os dados contêm valores ausentes.","metadata":{"id":"qEGXDle7k6QP"}},{"cell_type":"code","source":"train_tweets.isnull().mean()","metadata":{"id":"eJbeUHq8k6QP","outputId":"2d0360d6-0e6f-432e-e345-07d257c14c4e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(train_tweets_vectorized_media.isnull().mean())","metadata":{"id":"M5hmjajUk6QR","outputId":"982500c4-5d38-48ac-8174-02ebcfe27c3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(train_tweets_vectorized_text.isnull().mean())","metadata":{"id":"ixrfWwMFk6QR","outputId":"0b0190a6-c256-4254-c741-b91e9908833a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(user_vectorized_descriptions.isnull().mean())","metadata":{"id":"bv5Q4Z4Qk6QS","outputId":"4917df30-4bac-4c09-c5e1-9a8217f09b7a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(user_vectorized_profile_images.isnull().mean())","metadata":{"id":"wHu1i5E3k6QS","outputId":"4a00807d-59ba-44e2-e71f-5d3a94dc0d4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(users.isnull().mean())","metadata":{"id":"nu72QyJuk6QS","outputId":"013372ae-6f03-40f8-de8b-c2e2d544d8e0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Temos que apenas a variável 'tweet_topic_ids' possui valores ausentes, porém iremos descartá-la no decorrer do projeto.","metadata":{"id":"Vh1BlW4mgjoH"}},{"cell_type":"markdown","source":"Abaixo temos algumas informações da base de dados.","metadata":{"id":"tVHvQa1Qk6QT"}},{"cell_type":"code","source":"train_tweets.info()","metadata":{"id":"q_me-dqQk6QT","outputId":"6d00dbc4-48cd-4be4-8cb0-5eadb0f74e62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tweets_vectorized_media.info()","metadata":{"id":"b_jMNDRhk6QU","outputId":"76f82f24-897f-4d5e-c9c6-65dab0277ca0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tweets_vectorized_text.info()","metadata":{"id":"LToqyONak6QU","outputId":"e4457159-afbb-464a-c747-db2caa31a6b0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_vectorized_descriptions.info()","metadata":{"id":"0dT40-TBk6QU","outputId":"3de86a1c-b281-45dc-b297-343d1901ff63"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_vectorized_profile_images.info()","metadata":{"id":"GzvFH-tYk6QV","outputId":"15f5004b-7388-49a6-94df-9c191562620a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"users.info()","metadata":{"id":"dfc89v4ak6QV","outputId":"614fe1a6-f195-437f-b600-b064a79ab615"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1 Análise descritiva","metadata":{"id":"Kvs5rxcok6QW"}},{"cell_type":"markdown","source":"Nesta seção apresenta-se uma análise descritiva dos dados.","metadata":{"id":"GP6rNG1mk6QW"}},{"cell_type":"markdown","source":"### 1.1 Tweets","metadata":{"id":"Siv26XWDk6QW"}},{"cell_type":"markdown","source":"Aqui, faremos uma análise descritiva dos dados relacionados aos Tweets. \nOs dados referentes aos tweets estão separados em 3 CSVs, sendo eles:\n\n* 1 - Dados dos tweets;\n* 2 - Dados referentes as mídias presentes nos tweets (essas informações são dados em forma vetorizada);\n* 3 - Dados referentes ao texto presente nos tweets (essas informações são dados em forma vetorizada).","metadata":{"id":"Go514xBMk6QW"}},{"cell_type":"markdown","source":"Assim, vejamos a proporção da variável resposta, isto é, como ela está distribuída.","metadata":{"id":"IAm_2_Cjk6QX"}},{"cell_type":"code","source":"df = train_tweets\ndf.head()\nx,y = 'index', 'virality'\n\ndf1 = df[y].value_counts(normalize=True)\ndf1 = df1.mul(100)\ndf1 = df1.rename('percent').reset_index()\n\ng = sns.catplot(x=x,y='percent',kind='bar',data=df1)\ng.ax.set_ylim(0,50)\n\nfor p in g.ax.patches:\n    txt = str(p.get_height().round(2)) + '%'\n    txt_x = p.get_x() \n    txt_y = p.get_height()\n    g.ax.text(txt_x,txt_y,txt)\n    \nplt.title(\"Distribuição da resposta - virality\")\nplt.xlabel(\"virality\")\nplt.ylabel(\"Proporção\")\nplt.show()","metadata":{"id":"xvKApAwRk6QX","outputId":"6220f794-ab3d-47b3-e813-dcc256252065"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos ver que os dados apresentam um desbalanceamento considerável em relação à variável resposta, sendo tweets mais virais menos frequêntes que tweets menos virais na base.","metadata":{"id":"igeDLMr6g_fp"}},{"cell_type":"markdown","source":"Ainda, apresentamos algumas medidas destritivas do dataset train_tweets.","metadata":{"id":"gl_bkwzok6QY"}},{"cell_type":"code","source":"train_tweets.describe()","metadata":{"id":"7qiTD5mFk6QY","outputId":"fb4a3e2c-a086-4feb-8bf7-319f2c4cd3c3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Em seguida, fazemos alguns boxplots para verificação de outliers e para a averiguação de como estão destribuídos os atritudos numéricos do data.frame train_tweets.","metadata":{"id":"vcdcEciZk6QZ"}},{"cell_type":"code","source":"for col in ['tweet_created_at_year', 'tweet_created_at_hour', 'tweet_hashtag_count',\n            'tweet_url_count',  'tweet_mention_count']:\n    sns.boxplot(x = train_tweets[col])\n    plt.show()","metadata":{"id":"VOWlTs-nk6QZ","outputId":"93e1816c-5f29-40ac-f7f2-94f58d73bae0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Com os box-plots, não encontramos nenhum valor outlier que esteja fora do normal de acordo com os atributos.","metadata":{"id":"qeefX1johZlS"}},{"cell_type":"markdown","source":"Abaixo apresentamos uma matriz de correlação dos dados dos tweets.","metadata":{"id":"saa9S6Cmk6QZ"}},{"cell_type":"code","source":"plt.figure(figsize=(13, 9))\n       \ncorrMatrix = train_tweets.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","metadata":{"id":"bBKcBVSAk6Qa","outputId":"3e512617-3c6e-45be-b3da-7067ba7ac738"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Não encontramos nenhuma correlação muito forte entre as features.","metadata":{"id":"Qh-cvS10h42-"}},{"cell_type":"markdown","source":"### 1.1 Users","metadata":{"id":"uVASC3KLk6Qa"}},{"cell_type":"markdown","source":"Agora, será desenvolvida uma análise para os dados relacionados aos usuários.","metadata":{"id":"Rnt22rRok6Qb"}},{"cell_type":"markdown","source":"Vejamos abaixo as dimensões dos datasets relacionados aos Usuários.","metadata":{"id":"oigGPwvTk6Qb"}},{"cell_type":"code","source":"user_vectorized_profile_images.shape","metadata":{"id":"-bFO4wjek6Qb","outputId":"f975b114-7af2-407e-f985-716c7c49f6f3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_vectorized_descriptions.shape","metadata":{"id":"EiJi4vrEk6Qb","outputId":"e7ae43d2-f28d-4d5d-a51e-d2a89b1e9f34"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"users.shape","metadata":{"id":"xMvT-X9Ak6Qc","outputId":"8cc6611d-100c-49d0-dc55-894649de7c3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"users","metadata":{"id":"BXOGW1Gxk6Qc","outputId":"199aeee6-7ac4-4cd7-cd6a-dd298902e29e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Segue abaixo a estimativa kernel das distribuições dos atributos relacionados aos usuários.","metadata":{"id":"zfelolCwiKI8"}},{"cell_type":"code","source":"for  col in ['user_like_count', 'user_followers_count',\n             'user_following_count', 'user_listed_on_count',\n             'user_tweet_count', 'user_created_at_year',\n             'user_created_at_month']:\n    sns.kdeplot(data=users, x = col, fill=\"stack\")\n    plt.show()","metadata":{"id":"-VgTuqhDk6Qc","outputId":"ed5a2f02-7c4b-4c20-ecd5-5b41364237cc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos que nenhum atributo possui uma distribuição muito singular. A maioria deles são distribuições de forma assimétrica à direita.\n","metadata":{"id":"0Uz0VzUzjMS6"}},{"cell_type":"markdown","source":"Abaixo temos a matriz de correlação dos atributos relacionados aos usuários.","metadata":{"id":"x3fZOSwYjcPA"}},{"cell_type":"code","source":"plt.figure(figsize=(13, 9))\n\ncorrMatrix = users.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","metadata":{"id":"TO_dPsYMk6Qd","outputId":"736e9506-c9e3-43d3-99f9-f0c61be65dc0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Juntando os dados","metadata":{"id":"_UW_tK92k6Qd"}},{"cell_type":"markdown","source":"#### 1.2.1 Funções","metadata":{}},{"cell_type":"markdown","source":"A função MergeMedias foi criada para fazer a média dos atributos das midias de cada tweet_id, pois não foi possível concatenar todas as midias de cada tweet. Se fizéssemos isso, haveria muitos atributos esparsos na matriz de atributos de tweets. Assim, decidimos trabalhar com a média dos tweets. Entretanto, não notamos melhoras nos classificadores e até fizemos um teste para verificar se apenas os atribubos médios das midias seriam capazes de idenficar tweets virais. Para a nossa surpresa, todos os classificadores testados predizeram apenas a viralidade \"1\". Logo, concluímos que os atributos das midias não acrescentam informação útil para a classificação da viralidade dos tweets.\n**Com base nesta análise, decidimos não trabalhar com os atributos das medias**.","metadata":{"id":"5AvgbmN8G643"}},{"cell_type":"code","source":"def MergeMedias(media,tweets):\n    tweet_id = tweets.tweet_id.values\n    \n    names = media.columns[1:]\n    tempmatrix = np.zeros((len(tweet_id),len(names)))   \n    num_imagens = [0]*len(tweet_id)\n    for i in media.index:\n        index = np.where(tweet_id == media[\"tweet_id\"].iloc[i])[0][0]\n        j = num_imagens[index]\n        tempmatrix[index,] += media.iloc[i,1:]\n        num_imagens[index] += 1\n        \n    for i in np.arange(tweet_id.shape[0]):\n        if (num_imagens[i]>1):\n            tempmatrix[i,:] /= num_imagens[i]\n\n    newmedia = pd.DataFrame(tempmatrix, index=range(tempmatrix.shape[0]), columns=names)\n    newmedia[\"tweet_id\"] = tweet_id\n\n    return newmedia","metadata":{"id":"TCgcULTAk6Qe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2.2 User","metadata":{"id":"j-fXlAe8k6Qe"}},{"cell_type":"markdown","source":"Combina os dados dos usuários antes de juntar com os dados dos tweets.","metadata":{"id":"9W6MQmfeJDwZ"}},{"cell_type":"code","source":"#Junta as tabelas\nusers = users.merge(user_vectorized_descriptions, on = \"user_id\", how = \"left\")\nusers = users.merge(user_vectorized_profile_images, on = \"user_id\", how = \"left\")","metadata":{"id":"iPNSTPmwk6Qe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2.3 Combinando dados","metadata":{"id":"vywkq307k6Qe"}},{"cell_type":"code","source":"#Juntando dados dos tweets\ntrain_tweets = train_tweets.merge(train_tweets_vectorized_text, on = \"tweet_id\", how = \"left\")\ntest_tweets = test_tweets.merge(test_tweets_vectorized_text, on = \"tweet_id\", how = \"left\")","metadata":{"id":"-5if3ayjk6Qf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.merge(train_tweets, users, how='left', left_on='tweet_user_id', right_on='user_id')\ntrain.drop([\"tweet_id\", \"user_id\", \"tweet_user_id\"], axis = 1, inplace = True)\n\ntest = pd.merge(test_tweets, users, how='left', left_on='tweet_user_id', right_on='user_id')\ntestId = test.loc[:, \"tweet_id\"]\ntest.drop([\"tweet_id\", \"user_id\", \"tweet_user_id\"], axis = 1, inplace = True)","metadata":{"id":"1mI4UE-Wk6Qf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como temos que a variável `tweet_topic_ids` é categórica com muitas classes e pretendemos transforma-la em Dummies, optamos por excluí-la da análise, pois ela retornará muitas colunas com valores 0.","metadata":{"id":"qGAcobZ9Ja7E"}},{"cell_type":"code","source":"#Remove a coluna tweet_topic_ids, pq tava gastando muito tempo pra tentar limpar ela\n# e já conseguimos um resultado muito satisfatório sem ela, como será visto na sequência.\ntrain.drop(\"tweet_topic_ids\", axis = 1, inplace = True)\ntest.drop(\"tweet_topic_ids\", axis = 1, inplace = True)","metadata":{"id":"zriQtXWGk6Qf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Converte a única variável categórica do nosso conjunto em numérica, usando o comando `get_dummies`.","metadata":{"id":"hxUbtG8ZJ8fM"}},{"cell_type":"code","source":"#Dummy \ntrain = pd.concat([train, pd.get_dummies(train.tweet_attachment_class)], axis = 1) \ntrain.drop(\"tweet_attachment_class\", axis = 1, inplace = True)\n\ntest = pd.concat([test, pd.get_dummies(test.tweet_attachment_class)], axis = 1) \ntest.drop(\"tweet_attachment_class\", axis = 1, inplace = True)","metadata":{"id":"V_FnCzThk6Qf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col_name in train.columns: \n    print(col_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 Boxplot de alguns atributos em função da viralidade","metadata":{}},{"cell_type":"markdown","source":"#### 1.3.1 user_like_count","metadata":{}},{"cell_type":"code","source":"sns.boxplot(y='user_like_count', x='virality', \n                 data=train, \n                 palette=\"colorblind\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3.2 tweet_hashtag_count","metadata":{}},{"cell_type":"code","source":"sns.boxplot(y='tweet_hashtag_count', x='virality', \n                 data=train, \n                 palette=\"colorblind\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3.3 user_followers_count","metadata":{}},{"cell_type":"code","source":"sns.boxplot(y='user_followers_count', x='virality', \n                 data=train, \n                 palette=\"colorblind\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3.4 user_tweet_count","metadata":{}},{"cell_type":"code","source":"sns.boxplot(y='user_tweet_count', x='virality', \n                 data=train, \n                 palette=\"colorblind\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"id":"ptS8hAayk6Qg","outputId":"c16f5faf-86c2-4a6a-e85c-68289eea7213"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Redução de dimensionalidade e treinamento de modelos.","metadata":{"id":"g-nIFmx4ma3k"}},{"cell_type":"markdown","source":"Dado a dimensão dos dados ser muito grande, vamos utilizar alguns métodos diferentes visando reduzir a dimensionalidade. Após reduzirmos  a dimensionalidade, vamos trainar e validar os modelos, para ver qual método produz um resultado melhor.\n","metadata":{"id":"1IwEX-L_me1z"}},{"cell_type":"markdown","source":"### 2.1 PCA","metadata":{"id":"V2U9dxSzk6Qg"}},{"cell_type":"markdown","source":"Pela grande dimensionalidade do conjunto (3606 atributos), decidimos aplicar PCA em todos os atributos do nosso conjunto, como uma forma de reduzir a dimensionalidade para podermos utilizar em modelos e fazer as análises que queremos.","metadata":{"id":"fPvShhiOKWHr"}},{"cell_type":"code","source":"columns = train.columns.values\n\ny_columns = [\"virality\"]\nx_columns = [x for x in columns if x != \"virality\"]\n\nX = train[x_columns]\ny = train[y_columns]\n\n#PCA\npca = PCA(n_components = 500)\n\npca.fit(X)\n\nX_PCA = pca.transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2, stratify = y)\nX_train_PCA, X_test_PCA, y_train_PCA, y_test_PCA = train_test_split(X_PCA, y, test_size = 0.2, random_state = 2, stratify = y)","metadata":{"id":"3UtOaNDPk6Qg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Embora a variância seja explicada apenas pelos primeiros dez componentes, decidimos manter os 500 primeiros, pois o nosso receio era perder informação importante para a classificação que não foi captada pelos primeiros componentes. Como os atributos são muito diversos, incluindo desde informação dos tweets (texto), como informação do usuário (número de seguidores, número de likes, imagem do profile, entre outros), usando apenas os primeiros componentes limitaria o tipo de informação contida neles. Isso poderia deixar de fora informação importante, cuja variância é baixa. Verificamos essa hipótese testando os modelos com o número de componentes entre 10 e 500. Os melhores resultados no treinamento foi com 500 componentes.","metadata":{"id":"C91jOh2QKySt"}},{"cell_type":"code","source":"#Plot variância explicada acumulada\nplt.plot(np.cumsum(pca.explained_variance_ratio_[0:10]))\nplt.xlabel('Número de componentes')\nplt.ylabel('Variância explicada acumulada');","metadata":{"id":"fw2mAeeOk6Qh","outputId":"d3eed79c-805d-4cb7-ff0b-d198dfab3794"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como podemos ver pelo gráfico acima, temos que apenas com 2 componentes principais já conseguimos explicar quase 100% da variabilidade dos nossos dados.","metadata":{"id":"447kYWj_lcIz"}},{"cell_type":"markdown","source":"#### 2.1.1 Treinando e validando modelos com PCA","metadata":{"id":"aKG8Uapzk6Qh"}},{"cell_type":"markdown","source":"Ajuste dos hiperparâmetros do modelo XGBoost (comentado devido à demora em executá-lo).","metadata":{}},{"cell_type":"code","source":"# # hiperparametros para ajustar\n# param_grid = {\n#         'min_child_weight': [3, 9, 18],\n#         'gamma': [1, 3, 6, 9],\n#         'subsample': [0.6, 0.8, 1.0],\n#         'colsample_bytree': [0.6, 0.8, 1.0],\n#         'max_depth': [3, 4, 5],\n#         'n_estimators': [90,100,180],\n#         }\n\n# param_grid = {\n#         'n_estimators': [180],\n#         }\n\n# gs_ab = GridSearchCV(XGBClassifier(), param_grid = param_grid, n_jobs=8, scoring = \"accuracy\")\n\n# gs_ab.fit(X_train_PCA,y)\n\n# print('Melhores parâmetros:', gs_ab.best_params_)","metadata":{"id":"GiGARRQHk6Qh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #model = XGBClassifier(colsample_bytree = gs_ab.best_params_['colsample_bytree'], gamma = gs_ab.best_params_['gamma'],\n# #                      max_depth = gs_ab.best_params_['max_depth'], min_child_weight= gs_ab.best_params_['min_child_weight'],  subsample = gs_ab.best_params_['subsample'] )\n\nmodel = XGBClassifier(n_estimators = 180)\n\nmodel.fit(X_train, y_train.to_numpy().ravel())\n\ny_pred = model.predict(X_test)\nprint(\"Acc. Score: {}\".format(accuracy_score(y_test, y_pred)))","metadata":{"id":"P7Kt6UH7k6Qh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 LDA","metadata":{"id":"S5Sic6eWmL7O"}},{"cell_type":"markdown","source":"Ao utilizarmos o LDA, temos uma abordagem muito semelhante ao PCA, onde reduzimos a dimensionalidade por meio de uma transformação linear, porém, considerando as classes.","metadata":{"id":"Q8arIm_mnWay"}},{"cell_type":"code","source":"lda = LinearDiscriminantAnalysis(n_components=4)\n\nlda.fit(X_train,y_train)\ny_pred = lda.predict(X_test)\nprint(lda.score(X_test,y_test))","metadata":{"id":"zbUr_1WgmFYD","scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Univariate Feature Selection","metadata":{"id":"j-dKSUYQk6Qi"}},{"cell_type":"markdown","source":"Ainda levando em conta a grande dimensionalidade do conjunto, decidimos testar outra abordagem, que seria a de seleção de features univariada (dado que a recursiva nesse nosso caso seria inviável devido a quantidade de atributos). Primeiramente testamos a seleção de feature com a função score f_classif\n\n\n\n","metadata":{"id":"8XDV9Lt2xfwL"}},{"cell_type":"code","source":"X_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import mutual_info_classif \n\ntest = SelectKBest(score_func = f_classif, k=10)\nfit = test.fit(X_train, y_train)\n\nmask = fit.get_support() #list of booleans for selected features\nnew_feat = [] \nfor bool, feature in zip(mask, X_train.columns):\n    if bool:\n        new_feat.append(feature)\nprint('The best features are:{}'.format(new_feat))","metadata":{"id":"JbE2vdhsk6Qj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Segue abaixo os dados com as features que o método retornou","metadata":{"id":"aC5fn46g0xLO"}},{"cell_type":"code","source":"X_train1 = X_train.loc[:,new_feat]\nX_test1 = X_test.loc[:,new_feat]","metadata":{"id":"sLsUJGqik6Qj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Também utilizamos a função mutual_info_classif","metadata":{"id":"FjBeymvu03Ov"}},{"cell_type":"code","source":"test = SelectKBest(score_func = mutual_info_classif, k=10)\nfit = test.fit(X_train, y_train)\n\nmask = fit.get_support() #list of booleans for selected features\nnew_feat = [] \nfor bool, feature in zip(mask, X_train.columns):\n    if bool:\n        new_feat.append(feature)\nprint('The best features are:{}'.format(new_feat))","metadata":{"id":"HwbN8v2Fk6Qj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Segue abaixo os dados com as features que o método retornou","metadata":{"id":"z985u4ZI08AN"}},{"cell_type":"code","source":"X_train2 = X_train.loc[:,new_feat]\nX_test2 = X_test.loc[:,new_feat]","metadata":{"id":"ll4klA0Fk6Qj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3.1 Treinando e validando modelos com Feature Selection Univariada","metadata":{"id":"UFN0NLrOk6Qk"}},{"cell_type":"markdown","source":"Avaliando acurácia com os dados obtidos da feature selection com função \nf_classif","metadata":{"id":"77GA4t4XByyT"}},{"cell_type":"code","source":"model = XGBClassifier(n_estimators = 180)\nmodel.fit(X_train1, y_train.to_numpy().ravel())\ny_pred1 = model.predict(X_test1)\nprint(\"Acc. Score: {}\".format(accuracy_score(y_test, y_pred1)))","metadata":{"id":"zT3Vjs_Fk6Qk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Avaliando acurácia com os dados obtidos da feature selection com função \nmutual_info_classif","metadata":{"id":"5XWeexl_B-RI"}},{"cell_type":"code","source":"model = XGBClassifier(n_estimators = 180)\nmodel.fit(X_train2, y_train.to_numpy().ravel())\ny_pred2 = model.predict(X_test2)\nprint(\"Acc. Score: {}\".format(accuracy_score(y_test, y_pred2)))","metadata":{"id":"kx5y5eT-BflS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como podemos ver, a acurácia obtida utilizando a seleção de features (com ambas as funções) e o LDA se saiu inferior ao resultado de quando utilizamos o PCA, portanto, vamos seguir com a abordagem utilizando PCA para classificarmos no conjunto de teste.","metadata":{"id":"1P2D0cBeCD7X"}},{"cell_type":"markdown","source":"## 3. Resultado público do conjunto de teste","metadata":{"id":"G9cWru5QMN1l"}},{"cell_type":"markdown","source":"![alt text](df2cef56-2b05-4079-a724-ac331740e0cd.jpg \"Resultado do bitgrit\")","metadata":{"id":"d9eSH9oEMXhM"}},{"cell_type":"markdown","source":"## 4. Conclusão","metadata":{}},{"cell_type":"markdown","source":"Neste trabalho, desenvolvemos algumas estratégias para a classificação do nível de viralidade dos tweets. A maior dificuldade deste trabalho foi lidar com um número elevado de atributos, muito dos quais não tinham uma relação clara com o nível de viralidade dos tweets. Assim, testamos dois métodos de redução de dimensionalidade: PCA e LDA. O primeiro transforma as variáveis preditoras em componentes com as mairores variâncias dos dados de forma descrescente. O segundo é similar ao primeiro, mas também leva em conta as classes dos dados para determinar os eixos de transformação. Também testamos dois métodos de seleção de atributos f_classif e mutual_info_classif para usar nos classificadores.\n\nDevido ao volume enorme de dados do problema, optamos por testar apenas o modelo de classificação XGBoost no espaço reduzido de atributos. No caso da redução por LDA, também usamos esse método para classificar a viralidade dos tweets. Além disso, decidimos não usar os dados das mídias porque não conseguimos boa acurácia, usando apenas estes dados.\n\nCom estas estratégias, conseguimos uma acurácia de quase 67\\% no conjunto de teste separado do conjunto de treinamento usando o modelo XGBoost com PCA. O modelo LDA também apresentou um bom resultado, ficando em 66\\%. Os resultados do XGBoost com a seleção de atributos ficou em torno de 64\\%. Na submissão do `bitgrit`, conseguimos uma acurácia de 67.5\\% usando o modelo XGBoost com PCA de 500 componentes.\n\nFoi interessante participar de uma competição real, porém não tivemos tempo suficiente para explorar melhor outras estratégias de engenharia de atributos nem outros modelos de redução de atributos que levam em conta a variável preditora, como o próprio LDA. Além disso, como a base de dados era muito grande, muitos dos algoritmos que estávamos testando levaram horas para executar, limitando assim nossa capacidade de testar outras estratégias.","metadata":{}}]}